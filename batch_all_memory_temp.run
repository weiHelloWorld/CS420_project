#!/bin/bash
#PBS -q secondary
#PBS -l walltime=02:00:00
#PBS -l nodes=4:ppn=12:taub
#PBS -N project_cs420
#PBS -j oe

# Move to the current directory
module load intel/15.0 
module load valgrind 
cd $PBS_O_WORKDIR

# Define valgrind memory finding function
valgrind_result() {
    for i in $(ls massif.out.*)
    do 
        cat $i | grep mem_heap_B | sed 's/mem_heap_B=//' | sort -n | tail -1 
    done | sort -n | tail -2 | head -1
}

# It turns out that MPI is incorrectly allocating OpenMP procs and MPI ranks. 
# This code from MP4 (somehow) fixes the issue. 
module load mvapich2/2.1rc1-intel-15.0 
uniq ${PBS_NODEFILE} > /tmp/${USER}_NODEFILE
hst="-hostfile /tmp/${USER}_NODEFILE"
vg="valgrind --tool=massif"

# List of programs to be run
programs="MPI_cannon naive"

for program in $programs
do
    out_file_name="out_${program}_memory.txt"
    exec_program="./$program.exe"
    lscpu >> ${out_file_name}
    echo " " >> ${out_file_name}

    
    size=15000
    echo "------------------------------------------" >> ${out_file_name}
    echo "$program variable ranks" >> ${out_file_name}
    echo "------------------------------------------" >> ${out_file_name}

    for ranks in {1..10}
    do
        echo "Running ${exec_program} with ranks=${ranks}*${ranks}" >> ${out_file_name}
        export OMP_NUM_THREADS=1
        rm ./massif.out.*
        mpirun -np $((ranks*ranks)) ${hst} ${vg} ${exec_program} ${size} ${size} ${size} ${ranks} 4 1 1 >> ${out_file_name}
        valgrind_result >> ${out_file_name}
        echo " " >> ${out_file_name}
    done


done

rm ./massif.out.*

# Notes

